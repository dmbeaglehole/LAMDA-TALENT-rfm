{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d783af5d-677e-43e8-9707-30bd0dafe1a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9511c52-fa0f-47ae-85a3-0394040b718e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def get_dataset_info(base_path):\n",
    "    datasets = []\n",
    "    train_sizes = []\n",
    "    num_features = []\n",
    "    cat_features = []\n",
    "    num_tot_features = []\n",
    "    variances = []\n",
    "    num_classes = []\n",
    "    \n",
    "    # Get all directories\n",
    "    for dirname in os.listdir(base_path):\n",
    "        dir_path = os.path.join(base_path, dirname)\n",
    "        \n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(dir_path):\n",
    "            json_path = os.path.join(dir_path, 'info.json')\n",
    "            \n",
    "            # Check if info.json exists\n",
    "            if os.path.exists(json_path):\n",
    "                try:\n",
    "                    with open(json_path, 'r') as f:\n",
    "                        info = json.load(f)\n",
    "                        \n",
    "                    \n",
    "                    # Append values to respective lists\n",
    "                    datasets.append(dirname)\n",
    "                    train_sizes.append(info.get('train_size'))\n",
    "                    num_features.append(info.get('n_num_features'))\n",
    "                    cat_features.append(info.get('n_cat_features'))\n",
    "                    num_tot_features.append(info.get('n_num_features') + info.get('n_cat_features'))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {dirname}: {str(e)}\")\n",
    "            \n",
    "            if info['task_type'] == 'multiclass':\n",
    "                num_classes.append(info.get('n_classes'))\n",
    "            else:\n",
    "                num_classes.append(None)\n",
    "                \n",
    "            if info['task_type'] != 'regression':\n",
    "                variances.append(None)\n",
    "                continue\n",
    "                \n",
    "            if os.path.exists(json_path):\n",
    "                try:\n",
    "                    file_path = os.path.join(dir_path, 'y_test.npy')\n",
    "                    # Load the data with allow_pickle=True\n",
    "                    labels = np.load(file_path, allow_pickle=True)\n",
    "                    # print(\"labels\",labels)\n",
    "\n",
    "                    # Convert data to numeric type if possible\n",
    "                    if isinstance(labels[0], (list, tuple)):\n",
    "                        labels = np.array(labels, dtype=float)\n",
    "            \n",
    "                    variance = np.var(labels)\n",
    "                    variances.append(variance)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {dirname}: {str(e)}\")\n",
    "            \n",
    "    \n",
    "    return datasets, train_sizes, num_classes, num_features, cat_features, num_tot_features, variances\n",
    "\n",
    "# Usage\n",
    "base_path = './datasets'\n",
    "datasets, train_sizes, num_classes, num_features, cat_features, num_tot_features, variances = get_dataset_info(base_path)\n",
    "\n",
    "var_dict = {}\n",
    "for d, v in zip(datasets, variances):\n",
    "    var_dict[d] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4ea0e-5ec5-4bdf-851b-5f4d38673a32",
   "metadata": {},
   "source": [
    "## Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8e4bba-f758-431a-945b-a7b0991d89d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "reg_df = pd.read_csv('results_regression.csv')\n",
    "bin_df = pd.read_csv('results_binary_classification.csv')\n",
    "multi_df = pd.read_csv('results_multi-class_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be94310d-5d19-4cf3-8acf-2699160597fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_results(df, metric_stats, method='rfm', tag=''):\n",
    "    method_name = f'{method}{tag}'\n",
    "    # Add rfm column if it doesn't exist, initialize with NaN\n",
    "    if method not in df.columns:\n",
    "        df[method_name] = float('nan')  # or df['rfm'] = pd.NA\n",
    "        \n",
    "    # Go through each dataset in metric_stats\n",
    "    for dataset_name, metrics in metric_stats.items():\n",
    "        # Remove the '-rfm' suffix to match with DataFrame\n",
    "        base_name = dataset_name.replace(f'-{method}', '')\n",
    "        \n",
    "        # Check if this dataset exists in the DataFrame\n",
    "        if base_name in df['Dataset/Model'].values:\n",
    "            # Determine if it's classification based on presence of 'Accuracy' metric\n",
    "            is_classification = 'Accuracy' in metrics\n",
    "            \n",
    "            # Get the appropriate metric value\n",
    "            if is_classification:\n",
    "                metric_value = metrics['Accuracy']['mean']\n",
    "            else:\n",
    "                metric_value = metrics['RMSE']['mean']\n",
    "            \n",
    "            # Update the rfm value in the DataFrame\n",
    "            mask = df['Dataset/Model'] == base_name\n",
    "            df.loc[mask, f'{method_name}'] = metric_value\n",
    "            \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0351dd-14c6-4aa6-b504-19dd92d8e80c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from statistics import mean, stdev\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def load_results(results_dir='rfm_results'):\n",
    "    \"\"\"\n",
    "    Load all pickle files from the specified directory into a dictionary.\n",
    "    Each pickle file should contain results from different dataset-model combinations.\n",
    "    \n",
    "    Args:\n",
    "        results_dir (str): Directory containing the pickle files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with dataset-model combinations as keys and loaded data as values\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Convert to Path object for easier handling\n",
    "    results_path = Path(results_dir)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    if not results_path.exists():\n",
    "        raise FileNotFoundError(f\"Directory {results_dir} not found\")\n",
    "    \n",
    "    # Iterate through all pickle files in directory\n",
    "    for file_path in results_path.glob('*.pkl'):\n",
    "        try:\n",
    "            # Extract dataset and model type from filename\n",
    "            filename = file_path.stem  # Get filename without extension\n",
    "            \n",
    "            # Load pickle file\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                \n",
    "            # Store in results dictionary\n",
    "            # Using filename as key to maintain dataset-model relationship\n",
    "            results[filename] = {\n",
    "                'info': data['info'],\n",
    "                'args': data['args'],\n",
    "                'results': data['results'],\n",
    "                'time': data['time'],\n",
    "                'metric_name': data['metric_name']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_metric_stats(results_dict: Dict) -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Calculate the mean and standard deviation of each metric for each dataset-model combination.\n",
    "    \n",
    "    Args:\n",
    "        results_dict (dict): Dictionary containing the loaded results from pickle files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with dataset-model combinations as keys and metric statistics as values\n",
    "    \"\"\"\n",
    "    metric_stats = {}\n",
    "    \n",
    "    for filename, data in results_dict.items():\n",
    "        # Initialize storage for this dataset-model combination\n",
    "        metric_stats[filename] = {}\n",
    "        \n",
    "        # Get the results which contain metric tuples\n",
    "        results = data['results']\n",
    "        \n",
    "        # Skip if no results\n",
    "        if not results:\n",
    "            print(f\"Warning: No results found for {filename}\")\n",
    "            continue\n",
    "            \n",
    "        # Determine number of metrics in each tuple\n",
    "        num_metrics = len(results[0])\n",
    "        \n",
    "        # Calculate statistics for each metric position\n",
    "        for metric_idx in range(num_metrics):\n",
    "            try:\n",
    "                # Extract the metric at current position from all results\n",
    "                metric_values = [result[metric_idx] for result in results]\n",
    "                \n",
    "                # Get metric name\n",
    "                metric_name = f\"metric_{metric_idx}\"\n",
    "                if data.get('metric_name') and isinstance(data['metric_name'], (list, tuple)):\n",
    "                    metric_name = data['metric_name'][metric_idx]\n",
    "                \n",
    "                # Calculate statistics\n",
    "                metric_stats[filename][metric_name] = {\n",
    "                    'mean': mean(metric_values),\n",
    "                    'std': stdev(metric_values) if len(metric_values) > 1 else 0\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating statistics for metric {metric_idx} in {filename}: {str(e)}\")\n",
    "                metric_stats[filename][f\"metric_{metric_idx}\"] = {\n",
    "                    'mean': None,\n",
    "                    'std': None\n",
    "                }\n",
    "    \n",
    "    return metric_stats\n",
    "\n",
    "def print_metric_summary(metric_stats: Dict[str, Dict[str, Dict[str, float]]]) -> None:\n",
    "    \"\"\"\n",
    "    Print a formatted summary of the metric statistics for each dataset-model combination.\n",
    "    \n",
    "    Args:\n",
    "        metric_stats (dict): Dictionary containing the calculated metric statistics\n",
    "    \"\"\"\n",
    "    print(\"\\nMetric Statistics Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for filename, metrics in metric_stats.items():\n",
    "        print(f\"\\nDataset-Model: {filename}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for metric_name, stats in metrics.items():\n",
    "            print(f\"\\n{metric_name}:\")\n",
    "            if stats['mean'] is not None and stats['std'] is not None:\n",
    "                print(f\"  Mean: {stats['mean']:.4f}\")\n",
    "                print(f\"  Std:  {stats['std']:.4f}\")\n",
    "            else:\n",
    "                print(\"  Error calculating statistics\")\n",
    "    \n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load results\n",
    "        results_dict = load_results()\n",
    "        metric_stats = calculate_metric_stats(results_dict)\n",
    "\n",
    "        reg_df = add_results(reg_df, metric_stats)\n",
    "        bin_df = add_results(bin_df, metric_stats)\n",
    "        multi_df = add_results(multi_df, metric_stats)\n",
    "\n",
    "        results_dict = load_results('rfm_results_power_3_23')\n",
    "        metric_stats = calculate_metric_stats(results_dict)\n",
    "\n",
    "        # reg_df = add_results(reg_df, metric_stats, method='rfm', tag='_power')\n",
    "        # bin_df = add_results(bin_df, metric_stats, method='rfm', tag='_power')\n",
    "        # multi_df = add_results(multi_df, metric_stats, method='rfm', tag='_power')\n",
    "\n",
    "\n",
    "        results_dict = load_results('pfn-v2-results')\n",
    "        metric_stats = calculate_metric_stats(results_dict)\n",
    "\n",
    "        reg_df = add_results(reg_df, metric_stats, method='PFN-v2')\n",
    "        bin_df = add_results(bin_df, metric_stats, method='PFN-v2')\n",
    "        multi_df = add_results(multi_df, metric_stats, method='PFN-v2')\n",
    "\n",
    "\n",
    "        results_dict = load_results('kernel_results')\n",
    "        metric_stats = calculate_metric_stats(results_dict)\n",
    "\n",
    "        reg_df = add_results(reg_df, metric_stats, method='kernel')\n",
    "        bin_df = add_results(bin_df, metric_stats, method='kernel')\n",
    "        multi_df = add_results(multi_df, metric_stats, method='kernel')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65a657c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gmean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893fe121-2987-439f-bc96-de68cc0dfe2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_performance(reg_df, bin_df, multi_df, methods=None):\n",
    "    \"\"\"\n",
    "    Analyze performance metrics for specified methods across regression and classification tasks.\n",
    "    \n",
    "    Parameters:\n",
    "    reg_df, bin_df, multi_df: DataFrames containing performance data\n",
    "    methods: Optional list of method names to analyze. If None, analyzes all methods.\n",
    "    \n",
    "    Returns a DataFrame with average scores and ranks for each method.\n",
    "    Skips methods that aren't found in the data without raising an error.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Process regression datasets\n",
    "    if not reg_df.empty:\n",
    "        # Get method columns (excluding Dataset/Model)\n",
    "        all_method_cols = [col for col in reg_df.columns if col != 'Dataset/Model' and col != 'tabpfn' and col != 'dummy' and col != 'train_size']\n",
    "        method_cols = methods if methods else all_method_cols\n",
    "        method_cols = [col for col in method_cols if col in all_method_cols]\n",
    "\n",
    "        \n",
    "        # Calculate ranks for each row (smaller is better for RMSE)\n",
    "        try:\n",
    "            min_scores = reg_df[method_cols].min(axis=1)\n",
    "            max_scores = reg_df[method_cols].max(axis=1)\n",
    "\n",
    "            ranks = reg_df[all_method_cols].rank(axis=1, ascending=True)\n",
    "            for method in method_cols:\n",
    "                try:\n",
    "                    avg_rank = ranks[method].mean()\n",
    "                    avg_score = gmean(reg_df[method].values)\n",
    "                    avg_norm_score = ((reg_df[method] - min_scores) / (max_scores - min_scores)).mean()\n",
    "                    results.append({\n",
    "                        'Type': 'Regression',\n",
    "                        'Method': method,\n",
    "                        'Datasets': len(reg_df),\n",
    "                        'Metric': 'RMSE',\n",
    "                        'Average_Score': avg_score,\n",
    "                        'Average_Rank': avg_rank,\n",
    "                        'Average_Normalized_Score': avg_norm_score\n",
    "                    })\n",
    "                except KeyError:\n",
    "                    print(f\"Warning: Method '{method}' not found in regression data\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing regression data: {str(e)}\")\n",
    "    \n",
    "    # Process binary classification datasets\n",
    "    if not bin_df.empty:\n",
    "        all_method_cols = [col for col in bin_df.columns if col != 'Dataset/Model' and col != 'tabpfn' and col != 'dummy' and col != 'train_size']\n",
    "        method_cols = methods if methods else all_method_cols\n",
    "        method_cols = [col for col in method_cols if col in all_method_cols]\n",
    "\n",
    "        min_scores = bin_df[method_cols].min(axis=1)\n",
    "        max_scores = bin_df[method_cols].max(axis=1)\n",
    "        \n",
    "        # Calculate ranks for each row (larger is better for Accuracy)\n",
    "        try:\n",
    "            \n",
    "\n",
    "            ranks = bin_df[all_method_cols].rank(axis=1, ascending=False)\n",
    "            for method in method_cols:\n",
    "                try:\n",
    "                    avg_rank = ranks[method].mean()\n",
    "                    avg_score = bin_df[method].mean()\n",
    "                    avg_norm_score = ((bin_df[method] - min_scores) / (max_scores - min_scores)).mean()\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Type': 'Binary Classification',\n",
    "                        'Method': method,\n",
    "                        'Datasets': len(bin_df),\n",
    "                        'Metric': 'Accuracy',\n",
    "                        'Average_Score': avg_score,\n",
    "                        'Average_Rank': avg_rank,\n",
    "                        'Average_Normalized_Score': avg_norm_score\n",
    "                    })\n",
    "                except KeyError:\n",
    "                    print(f\"Warning: Method '{method}' not found in binary classification data\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing binary classification data: {str(e)}\")\n",
    "    \n",
    "    # Process multiclass classification datasets\n",
    "    if not multi_df.empty:\n",
    "        all_method_cols = [col for col in multi_df.columns if col != 'Dataset/Model' and col != 'tabpfn' and col != 'dummy' and col != 'train_size']\n",
    "        method_cols = methods if methods else all_method_cols\n",
    "        method_cols = [col for col in method_cols if col in all_method_cols]\n",
    "\n",
    "        min_scores = multi_df[method_cols].min(axis=1)\n",
    "        max_scores = multi_df[method_cols].max(axis=1)\n",
    "        \n",
    "        # Calculate ranks for each row (larger is better for Accuracy)\n",
    "        try:\n",
    "\n",
    "            ranks = multi_df[all_method_cols].rank(axis=1, ascending=False)\n",
    "            for method in method_cols:\n",
    "                try:\n",
    "                    avg_rank = ranks[method].mean()\n",
    "                    avg_score = multi_df[method].mean()\n",
    "                    avg_norm_score = ((multi_df[method] - min_scores) / (max_scores - min_scores)).mean()\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Type': 'Multiclass Classification',\n",
    "                        'Method': method,\n",
    "                        'Datasets': len(multi_df),\n",
    "                        'Metric': 'Accuracy',\n",
    "                        'Average_Score': avg_score,\n",
    "                        'Average_Rank': avg_rank,\n",
    "                        'Average_Normalized_Score': avg_norm_score\n",
    "                    })\n",
    "                except KeyError:\n",
    "                    print(f\"Warning: Method '{method}' not found in multiclass classification data\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing multiclass classification data: {str(e)}\")\n",
    "    \n",
    "    # Convert results to DataFrame and sort by Type and Average_Rank\n",
    "    if not results:\n",
    "        print(\"Warning: No valid results were generated\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df.sort_values(['Type', 'Average_Rank'])\n",
    "\n",
    "def print_performance_summary(results_df):\n",
    "    \"\"\"\n",
    "    Print a formatted summary of the performance analysis results with clear visual separation\n",
    "    between different task types.\n",
    "    \n",
    "    Parameters:\n",
    "    results_df: DataFrame containing performance analysis results\n",
    "    \"\"\"\n",
    "    # Define some formatting constants\n",
    "    SECTION_WIDTH = 120\n",
    "    DOUBLE_LINE = \"=\" * SECTION_WIDTH\n",
    "    SINGLE_LINE = \"-\" * SECTION_WIDTH\n",
    "    \n",
    "    print(\"\\nPERFORMANCE ANALYSIS SUMMARY\")\n",
    "    print(DOUBLE_LINE)\n",
    "    \n",
    "    total_datasets = 0\n",
    "    for task_type in results_df['Type'].unique():\n",
    "        task_results = results_df[results_df['Type'] == task_type]\n",
    "        \n",
    "        # Print section header\n",
    "        print(f\"\\n{task_type.upper()}\")\n",
    "        print(SINGLE_LINE)\n",
    "        \n",
    "        # Print metadata\n",
    "        print(f\"Datasets analyzed: {task_results['Datasets'].iloc[0]}\")\n",
    "        print(f\"Evaluation metric: {task_results['Metric'].iloc[0]}\\n\")\n",
    "        \n",
    "        total_datasets += task_results['Datasets'].iloc[0]\n",
    "        \n",
    "        # Print column headers\n",
    "        print(f\"{'Method':<25} {'Avg Rank':<15} {'Avg Score':<15} {'Avg Norm Score':<15}\")\n",
    "        print(SINGLE_LINE)\n",
    "        \n",
    "        # Print results for each method\n",
    "        for _, row in task_results.iterrows():\n",
    "            print(f\"{row['Method']:<25} {row['Average_Rank']:<15.2f} {row['Average_Score']:<15.4f} {row['Average_Normalized_Score']:<15.4f}\")\n",
    "        \n",
    "        print(SINGLE_LINE)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete. {len(results_df['Type'].unique())} task types evaluated. {len(results_df)} methods evaluated. {total_datasets} datasets evaluated.\")\n",
    "    print(DOUBLE_LINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b22683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf9d3b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset/Model</th>\n",
       "      <th>dummy</th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>knn</th>\n",
       "      <th>svm</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>catboost</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>mlp</th>\n",
       "      <th>...</th>\n",
       "      <th>grownet</th>\n",
       "      <th>tabr</th>\n",
       "      <th>modernNCA</th>\n",
       "      <th>mlp_plr</th>\n",
       "      <th>realmlp</th>\n",
       "      <th>excelformer</th>\n",
       "      <th>dnnr</th>\n",
       "      <th>rfm</th>\n",
       "      <th>PFN-v2</th>\n",
       "      <th>kernel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Dataset/Model, dummy, LinearRegression, knn, svm, xgboost, catboost, RandomForest, lightgbm, mlp, resnet, node, switchtab, tabnet, tangos, danets, ftt, autoint, dcn2, snn, tabtransformer, ptarl, grownet, tabr, modernNCA, mlp_plr, realmlp, excelformer, dnnr, rfm, PFN-v2, kernel]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_df[reg_df['rfm'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "952f7c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset/Model</th>\n",
       "      <th>dummy</th>\n",
       "      <th>LogReg</th>\n",
       "      <th>NCM</th>\n",
       "      <th>NaiveBayes</th>\n",
       "      <th>knn</th>\n",
       "      <th>svm</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>catboost</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>...</th>\n",
       "      <th>ptarl</th>\n",
       "      <th>grownet</th>\n",
       "      <th>tabr</th>\n",
       "      <th>modernNCA</th>\n",
       "      <th>mlp_plr</th>\n",
       "      <th>realmlp</th>\n",
       "      <th>excelformer</th>\n",
       "      <th>rfm</th>\n",
       "      <th>PFN-v2</th>\n",
       "      <th>kernel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>customer_satisfaction_in_airline</td>\n",
       "      <td>0.547313</td>\n",
       "      <td>0.903257</td>\n",
       "      <td>0.794079</td>\n",
       "      <td>0.812827</td>\n",
       "      <td>0.934324</td>\n",
       "      <td>0.902282</td>\n",
       "      <td>0.960733</td>\n",
       "      <td>0.963053</td>\n",
       "      <td>0.933603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95771</td>\n",
       "      <td>0.950503</td>\n",
       "      <td>0.963438</td>\n",
       "      <td>0.961685</td>\n",
       "      <td>0.960579</td>\n",
       "      <td>0.960848</td>\n",
       "      <td>0.959463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.949651</td>\n",
       "      <td>0.956498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Dataset/Model     dummy    LogReg       NCM  \\\n",
       "68  customer_satisfaction_in_airline  0.547313  0.903257  0.794079   \n",
       "\n",
       "    NaiveBayes       knn       svm   xgboost  catboost  RandomForest  ...  \\\n",
       "68    0.812827  0.934324  0.902282  0.960733  0.963053      0.933603  ...   \n",
       "\n",
       "      ptarl   grownet      tabr  modernNCA   mlp_plr   realmlp  excelformer  \\\n",
       "68  0.95771  0.950503  0.963438   0.961685  0.960579  0.960848     0.959463   \n",
       "\n",
       "    rfm    PFN-v2    kernel  \n",
       "68  NaN  0.949651  0.956498  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_df[bin_df['rfm'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "470acc6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset/Model</th>\n",
       "      <th>dummy</th>\n",
       "      <th>LogReg</th>\n",
       "      <th>NCM</th>\n",
       "      <th>NaiveBayes</th>\n",
       "      <th>knn</th>\n",
       "      <th>svm</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>catboost</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>...</th>\n",
       "      <th>ptarl</th>\n",
       "      <th>grownet</th>\n",
       "      <th>tabr</th>\n",
       "      <th>modernNCA</th>\n",
       "      <th>mlp_plr</th>\n",
       "      <th>realmlp</th>\n",
       "      <th>excelformer</th>\n",
       "      <th>rfm</th>\n",
       "      <th>PFN-v2</th>\n",
       "      <th>kernel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Credit_c</td>\n",
       "      <td>0.531750</td>\n",
       "      <td>0.733550</td>\n",
       "      <td>0.592800</td>\n",
       "      <td>0.433250</td>\n",
       "      <td>0.778650</td>\n",
       "      <td>0.631070</td>\n",
       "      <td>0.780113</td>\n",
       "      <td>0.797217</td>\n",
       "      <td>0.585353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775420</td>\n",
       "      <td>0.642300</td>\n",
       "      <td>0.820073</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0.789123</td>\n",
       "      <td>0.785010</td>\n",
       "      <td>0.784137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701387</td>\n",
       "      <td>0.765350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rain_in_Australia</td>\n",
       "      <td>0.758387</td>\n",
       "      <td>0.835453</td>\n",
       "      <td>0.717998</td>\n",
       "      <td>0.798776</td>\n",
       "      <td>0.833116</td>\n",
       "      <td>0.822116</td>\n",
       "      <td>0.852835</td>\n",
       "      <td>0.868922</td>\n",
       "      <td>0.765454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859508</td>\n",
       "      <td>0.817778</td>\n",
       "      <td>0.860901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.862693</td>\n",
       "      <td>0.869579</td>\n",
       "      <td>0.850541</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.840373</td>\n",
       "      <td>0.857349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>accelerometer</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.339499</td>\n",
       "      <td>0.339956</td>\n",
       "      <td>0.499493</td>\n",
       "      <td>0.732950</td>\n",
       "      <td>0.353039</td>\n",
       "      <td>0.734745</td>\n",
       "      <td>0.723796</td>\n",
       "      <td>0.676224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736425</td>\n",
       "      <td>0.570025</td>\n",
       "      <td>0.744326</td>\n",
       "      <td>0.746222</td>\n",
       "      <td>0.738603</td>\n",
       "      <td>0.739623</td>\n",
       "      <td>0.736030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.567385</td>\n",
       "      <td>0.736054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>walking-activity</td>\n",
       "      <td>0.147253</td>\n",
       "      <td>0.271035</td>\n",
       "      <td>0.275521</td>\n",
       "      <td>0.355007</td>\n",
       "      <td>0.630964</td>\n",
       "      <td>0.245488</td>\n",
       "      <td>0.651941</td>\n",
       "      <td>0.661738</td>\n",
       "      <td>0.615948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643312</td>\n",
       "      <td>0.410667</td>\n",
       "      <td>0.671296</td>\n",
       "      <td>0.677437</td>\n",
       "      <td>0.650020</td>\n",
       "      <td>0.659211</td>\n",
       "      <td>0.651935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.639468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset/Model     dummy    LogReg       NCM  NaiveBayes       knn  \\\n",
       "3            Credit_c  0.531750  0.733550  0.592800    0.433250  0.778650   \n",
       "10  Rain_in_Australia  0.758387  0.835453  0.717998    0.798776  0.833116   \n",
       "14      accelerometer  0.333322  0.339499  0.339956    0.499493  0.732950   \n",
       "72   walking-activity  0.147253  0.271035  0.275521    0.355007  0.630964   \n",
       "\n",
       "         svm   xgboost  catboost  RandomForest  ...     ptarl   grownet  \\\n",
       "3   0.631070  0.780113  0.797217      0.585353  ...  0.775420  0.642300   \n",
       "10  0.822116  0.852835  0.868922      0.765454  ...  0.859508  0.817778   \n",
       "14  0.353039  0.734745  0.723796      0.676224  ...  0.736425  0.570025   \n",
       "72  0.245488  0.651941  0.661738      0.615948  ...  0.643312  0.410667   \n",
       "\n",
       "        tabr  modernNCA   mlp_plr   realmlp  excelformer  rfm    PFN-v2  \\\n",
       "3   0.820073   0.798600  0.789123  0.785010     0.784137  NaN  0.701387   \n",
       "10  0.860901        NaN  0.862693  0.869579     0.850541  NaN  0.840373   \n",
       "14  0.744326   0.746222  0.738603  0.739623     0.736030  NaN  0.567385   \n",
       "72  0.671296   0.677437  0.650020  0.659211     0.651935  NaN       NaN   \n",
       "\n",
       "      kernel  \n",
       "3   0.765350  \n",
       "10  0.857349  \n",
       "14  0.736054  \n",
       "72  0.639468  \n",
       "\n",
       "[4 rows x 35 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_df[multi_df['rfm'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d874aef-471d-4403-8461-673961911cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Then drop rows where 'rfm' is still NA\n",
    "reg_df = reg_df.dropna(subset=['rfm'])\n",
    "bin_df = bin_df.dropna(subset=['rfm'])\n",
    "multi_df = multi_df.dropna(subset=['rfm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7b5ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try:\n",
    "#     reg_df = reg_df.dropna(subset=['rfm_power'])\n",
    "#     bin_df = bin_df.dropna(subset=['rfm_power'])\n",
    "#     multi_df = multi_df.dropna(subset=['rfm_power'])\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Get numeric columns only\n",
    "\n",
    "# Impute missing values with row means for numeric columns only\n",
    "numeric_cols = reg_df.select_dtypes(include=['float32', 'float64', 'int64']).columns\n",
    "reg_df[numeric_cols] = reg_df[numeric_cols].apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "\n",
    "numeric_cols = bin_df.select_dtypes(include=['float32', 'float64', 'int64']).columns\n",
    "bin_df[numeric_cols] = bin_df[numeric_cols].apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "\n",
    "numeric_cols = multi_df.select_dtypes(include=['float32', 'float64', 'int64']).columns\n",
    "multi_df[numeric_cols] = multi_df[numeric_cols].apply(lambda row: row.fillna(row.mean()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3e22747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map datasets to train sizes\n",
    "train_size_dict = dict(zip(datasets, train_sizes))\n",
    "\n",
    "# Add train_size column to reg_df, bin_df, and multi_df\n",
    "for df in [reg_df, bin_df, multi_df]:\n",
    "    df['train_size'] = df['Dataset/Model'].map(train_size_dict)\n",
    "\n",
    "num_classes_dict = dict(zip(datasets, num_classes))\n",
    "\n",
    "# Add train_size column to reg_df, bin_df, and multi_df\n",
    "for df in [multi_df]:\n",
    "    df['num_classes'] = df['Dataset/Model'].map(num_classes_dict)\n",
    "\n",
    "reg_df_final = reg_df\n",
    "bin_df_final = bin_df\n",
    "multi_df_final = multi_df\n",
    "\n",
    "view_large_datasets = True\n",
    "if view_large_datasets:\n",
    "    reg_df_final = reg_df[reg_df['train_size'] > 10_000]\n",
    "    bin_df_final = bin_df[bin_df['train_size'] > 10_000]\n",
    "    multi_df_final = multi_df[multi_df['train_size'] > 10_000]\n",
    "\n",
    "# view_small_datasets = False\n",
    "# if view_small_datasets:\n",
    "#     reg_df_final = reg_df[reg_df['train_size'] < 10_000]\n",
    "#     bin_df_final = bin_df[bin_df['train_size'] < 10_000]\n",
    "#     multi_df_final = multi_df[multi_df['train_size'] < 10_000]\n",
    "\n",
    "# multi_df_final = multi_df[multi_df['num_classes'] > 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf14051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c6c201e-00d8-483e-ae23-66963e80723c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# methods = ['dummy', 'LogReg', 'NCM', 'NaiveBayes', 'knn', 'svm',\n",
    "#        'xgboost', 'catboost', 'RandomForest', 'lightgbm', 'tabpfn', 'mlp',\n",
    "#        'resnet', 'node', 'switchtab', 'tabnet', 'tabcaps', 'tangos', 'danets',\n",
    "#        'ftt', 'autoint', 'dcn2', 'snn', 'tabtransformer', 'ptarl', 'grownet',\n",
    "#        'tabr', 'modernNCA', 'mlp_plr', 'realmlp', 'excelformer', 'rfm']\n",
    "methods = ['PFN-v2','catboost','xgboost','rfm','mlp','realmlp','lightgbm','svm','RandomForest','kernel']#, 'rfm_power']\n",
    "results_df = analyze_performance(reg_df_final, bin_df_final, multi_df_final, methods=methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58f7dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PERFORMANCE ANALYSIS SUMMARY\n",
      "========================================================================================================================\n",
      "\n",
      "BINARY CLASSIFICATION\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Datasets analyzed: 26\n",
      "Evaluation metric: Accuracy\n",
      "\n",
      "Method                    Avg Rank        Avg Score       Avg Norm Score \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "rfm                       5.92            0.8377          0.9182         \n",
      "realmlp                   7.50            0.8346          0.8091         \n",
      "PFN-v2                    10.44           0.8292          0.8034         \n",
      "catboost                  10.50           0.8215          0.8059         \n",
      "lightgbm                  10.52           0.8213          0.7725         \n",
      "xgboost                   12.12           0.8192          0.7432         \n",
      "mlp                       12.96           0.8278          0.7114         \n",
      "kernel                    14.37           0.8261          0.7085         \n",
      "RandomForest              19.40           0.8007          0.4988         \n",
      "svm                       21.54           0.7977          0.3265         \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "MULTICLASS CLASSIFICATION\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Datasets analyzed: 12\n",
      "Evaluation metric: Accuracy\n",
      "\n",
      "Method                    Avg Rank        Avg Score       Avg Norm Score \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "realmlp                   5.25            0.8310          0.9794         \n",
      "xgboost                   11.33           0.8020          0.8905         \n",
      "rfm                       11.33           0.8003          0.8804         \n",
      "catboost                  12.50           0.7848          0.8125         \n",
      "mlp                       13.75           0.8015          0.8171         \n",
      "lightgbm                  14.25           0.7894          0.7938         \n",
      "kernel                    17.17           0.7822          0.8151         \n",
      "PFN-v2                    19.38           0.7578          0.7480         \n",
      "RandomForest              22.25           0.7195          0.5312         \n",
      "svm                       28.50           0.6218          0.1752         \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "REGRESSION\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Datasets analyzed: 27\n",
      "Evaluation metric: RMSE\n",
      "\n",
      "Method                    Avg Rank        Avg Score       Avg Norm Score \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "realmlp                   5.15            23.3656         0.0516         \n",
      "rfm                       5.33            22.2729         0.0408         \n",
      "catboost                  5.85            25.7174         0.0406         \n",
      "PFN-v2                    6.89            24.8381         0.0968         \n",
      "lightgbm                  8.52            27.1600         0.0765         \n",
      "xgboost                   9.81            27.3380         0.0834         \n",
      "kernel                    13.89           28.4371         0.1768         \n",
      "RandomForest              14.52           29.0361         0.1747         \n",
      "mlp                       16.52           32.3823         0.2893         \n",
      "svm                       24.89           49.4658         0.9304         \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Analysis complete. 3 task types evaluated. 30 methods evaluated. 65 datasets evaluated.\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_performance_summary(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd66b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a7843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daniel_jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
